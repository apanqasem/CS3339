{"cells":[{"cell_type":"markdown","id":"96b88de4","metadata":{"id":"96b88de4"},"source":["## OpenMP Performance Scalability \n","[D2] Heterogeneous Programming with OpenMP  \n","Apan Qasem [\\<apan@txstate.edu\\>](apan@txstate.edu)\n","\n","### Description\n","\n","This interactive demo discusses the importance and significance of thread count in OpenMP\n","applications. It is assumed that students know how to write a Hello World program in OpenMP (e.g.,\n","have completed the ([Hello World in OpenMP](demo_hello_world.md)) tutorial. \n","\n","The demo also introduces the `parallel for` directive. A simple matrix-scalar multiplication code is\n","used as a running example. \n","\n","### Outline \n","\n","   * [The OpenMP Hello World Program](#hello)\n","   * [Parallelizing with `parallel for`](#pragma) \n","   * [Thread Count ans Scalability](#thread_count)\n","\n","\n","### <a name=\"hello\"></a>The OpenMP Hello World Program\n","\n","Below is the Hello World program with OpenMP parallelization that we wrote in our previous tutorial\n","([Hello World in OpenMP](demo_hello_world.md)). The code has been updated to read in the desired number threads from a command-line parameter. As noted in the previous tutorial, the `%%writefile hello.c` command will save the  file in the current directory."]},{"cell_type":"code","execution_count":null,"id":"264c5589","metadata":{"id":"264c5589"},"outputs":[],"source":["%%writefile hello.c\n","#include<stdio.h>\n","#include<stdlib.h>  // for atoi \n","#include<omp.h>\n","\n","int main(int argc, char* argv[]) {\n","\n","  int num_threads;\n","  if (argc <= 1)\n","    num_threads = 1;\n","  else\n","    num_threads = atoi(argv[1]);\n","\n","  omp_set_num_threads(num_threads);\n","  #pragma omp parallel \n","  {\n","    int ID = omp_get_thread_num();\n","    printf(\"Hello World from %d!\\n\", ID);\n","    printf(\"Goodbye World from %d!\\n\", ID);\n","  }\n","  return 0;\n","} "]},{"cell_type":"markdown","id":"f8040bae","metadata":{"id":"f8040bae"},"source":["Let us re-run this sequential version and time the run."]},{"cell_type":"code","execution_count":null,"id":"b9714673","metadata":{"scrolled":true,"id":"b9714673"},"outputs":[],"source":["!gcc -o hello -fopenmp hello.c\n","!time ./hello 1"]},{"cell_type":"markdown","id":"14b5dbfc","metadata":{"id":"14b5dbfc"},"source":["The Linux `time` command doesn't really give us satisfactory resolution for measuring the\n","performance of this _tiny_ program. We can use\n","[`perf`](https://perf.wiki.kernel.org/index.php/Main_Page) to get better measurements."]},{"cell_type":"markdown","id":"f50f45dd","metadata":{"id":"f50f45dd"},"source":["Now, let's run the code with 2 threads."]},{"cell_type":"code","execution_count":null,"id":"99ab7731","metadata":{"id":"99ab7731"},"outputs":[],"source":["!time ./hello 2"]},{"cell_type":"markdown","id":"a5b4ce3e","metadata":{"id":"a5b4ce3e"},"source":["_How much performance improvement do we get by running this code in parallel?_\n","\n","We can't really tell (probably none). This very simple code does not run for a sufficient amount of time for us to do any kind of performance analysis, at least not without tools that give us better timing resolution. \n","\n","### <a name=\"pragma\"></a>Parallelizing with `parallel for`\n","\n","Let's look at a code that is slightly more complex."]},{"cell_type":"code","execution_count":null,"id":"2fbf24d3","metadata":{"id":"2fbf24d3"},"outputs":[],"source":["%%writefile scale.c\n","#include<stdio.h>\n","#include<stdlib.h>\n","#include<sys/time.h>\n","\n","#include <omp.h>\n","\n","#define REPS 100\n","\n","double t0;\n","double mysecond() {\n","  struct timeval tp;\n","  struct timezone tzp;\n","  int i;\n","\n","  i = gettimeofday(&tp,&tzp);\n","  return ( (double) tp.tv_sec + (double) tp.tv_usec * 1.e-6 );\n","}\n","\n","int main(int argc, char *argv[]) {\n","  float **a, **b;\n","  \n","  int M = atoi(argv[1]);\n","  int N = atoi(argv[2]);\n","\n","  omp_set_num_threads(N);\n","\n","  a = (float **) malloc(sizeof(float *) * M);\n","  b = (float **) malloc(sizeof(float *) * M);\n","  \n","  int i, j, k;\n","  for (i = 0; i < M; i++) {\n","    a[i] = (float *) malloc(sizeof(float) * M);\n","    b[i] = (float *) malloc(sizeof(float) * M);\n","  }\n","\n","  for (j = 0; j < M; j++)\n","    for (i = 0; i < M; i++)\n","      b[i][j] = i + j;\n","\n","  t0 = mysecond();\n","#pragma omp parallel for \n","  for (int k = 0; k < REPS; k++) {\n","    for (int j = 0; j < M; j++) \n","      for (int i = 0; i < M; i++)\n","        a[i][j] = b[i][j] * 17;\n","  }\n","\n","  t0 = (mysecond() - t0) * 1.e3;\n","\n","  /* print an arbirtrary value from the result array */\n","  printf(\"result = %3.2f\\n\", a[17][17]);\n","  printf(\"parallel loop = %3.2f ms\\n\", t0);\n","\n","  return 0;\n","\n","}"]},{"cell_type":"markdown","id":"46eaed3c","metadata":{"id":"46eaed3c"},"source":["The above program scales the values in an array by a constant factor. The loop is parallelized with the\n","`parallel for` directive. This directive is an extension of the `parallel` directive and is applied\n","exclusively to the *next* for loop. The `parallel for` directive will equally divide the iterations\n","of the loop and run them in parallel. The number of threads to be created is passed via a command-line\n","argument. There's a built-in timer to record the execution time of the parallel loop. \n","\n","\n","\n","### <a name=\"thread_count\"></a>Thread Count and Scalability \n","\n","Let's build and execute the sequential version of the code."]},{"cell_type":"code","execution_count":null,"id":"6dd1c349","metadata":{"id":"6dd1c349"},"outputs":[],"source":["!g++ -o scale scale.c -fopenmp\n","!./scale 1000 1"]},{"cell_type":"markdown","id":"57a5be3c","metadata":{"id":"57a5be3c"},"source":["Let's run it with 2 threads."]},{"cell_type":"code","execution_count":null,"id":"8117a6aa","metadata":{"id":"8117a6aa"},"outputs":[],"source":["!./scale 1000 2"]},{"cell_type":"markdown","id":"a3429e7e","metadata":{"id":"a3429e7e"},"source":["The parallel version runs significantly faster. However note, even with this very simple code we are\n","not able to double the performance when we increase the number of threads from 1 to 2. \n","\n","#### Why?\n","\n","See [Heterogeneous Computing: Elementary Notions](https://github.com/TeachingUndergradsCHC/modules/tree/master/Fundamentals/elementary_notions) for one explanation. \n","\n","Let's check the number of available cores on this system\n"]},{"cell_type":"code","execution_count":null,"id":"90084192","metadata":{"scrolled":true,"id":"90084192"},"outputs":[],"source":["!lscpu | head -6"]},{"cell_type":"markdown","id":"16d7e0b8","metadata":{"id":"16d7e0b8"},"source":["Now, we will try to maximize the parallelization by running the code with a thread count that matches the number of logical cores (hardware threads). OpenMP would generally pick this thread count for this system if we did not specify the it ourselves."]},{"cell_type":"code","execution_count":null,"id":"018f7d97","metadata":{"id":"018f7d97"},"outputs":[],"source":["!./scale 1000 4"]},{"cell_type":"markdown","id":"033561b1","metadata":{"id":"033561b1"},"source":["_Does it give us more performance?_\n","\n","On most systems we will see an increase in performance. But not proportional to the number of threads used. \n","\n","_What if we kept on increasing the number of threads, do we expect to get more parallelism?_"]},{"cell_type":"code","execution_count":null,"id":"a2801b1a","metadata":{"id":"a2801b1a"},"outputs":[],"source":["!./scale 1000 32\n","!./scale 1000 64\n","!./scale 1000 128"]},{"cell_type":"markdown","id":"f8868c4a","metadata":{"id":"f8868c4a"},"source":["#### Does this performance pattern reminds us of something? \n","\n","This program becomes [compute-bound](https://en.wikipedia.org/wiki/CPU-bound) when the number of\n","threads is substantially higher than the available processing cores. At that point increasing the\n","number of threads doesn't give us any benefits (in fact in some cases it can actually hurt due to\n","thread creation overhead). \n","\n","The ideal number of threads for a given program depends on many factors. Often some fine-tuning is\n","necessary. \n","\n","### Exercise \n","\n","Compile and run the `matrix-scale` code on your own machine with increasing number of\n","threads. What is the ideal thread count?"]}],"metadata":{"kernelspec":{"display_name":"Bash","language":"bash","name":"bash"},"language_info":{"codemirror_mode":"shell","file_extension":".sh","mimetype":"text/x-sh","name":"bash"},"colab":{"name":"scalability.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}